{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7b1be4",
   "metadata": {},
   "source": [
    "# Post-Training Toolkit - Live Output Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927d4c2",
   "metadata": {},
   "source": [
    "## Part 1: The Problem\n",
    "\n",
    "**Training bugs waste compute and money.**\n",
    "\n",
    "Typical scenario:\n",
    "```\n",
    "Train for 8 hours ‚Üí Check metrics ‚Üí Something's wrong ‚Üí \n",
    "Dig through logs ‚Üí Find the bug ‚Üí Restart\n",
    "\n",
    "Cost: 8 hours √ó 8 GPUs √ó $2.50/hr = $160 wasted\n",
    "```\n",
    "\n",
    "**Common failure modes:**\n",
    "- KL divergence explosion\n",
    "- Importance sampling collapse (‚Üê what we'll see)\n",
    "- Reward hacking\n",
    "- Policy collapse\n",
    "\n",
    "**Most bugs show early signals, but we only look after training ends.**\n",
    "\n",
    "**PTT solution:** Monitor during training and catch bugs as they happen.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f2c730",
   "metadata": {},
   "source": [
    "## Part 2: The Solution - Just 3 Lines\n",
    "\n",
    "```python\n",
    "from post_training_toolkit import DiagnosticsCallback\n",
    "\n",
    "# Add to any TRL trainer (PPO, DPO, SFT, ORPO...)\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    callbacks=[DiagnosticsCallback(\n",
    "        enable_live_warnings=True,    # ‚Üê Live alerts in terminal\n",
    "        stop_on_critical=True,         # ‚Üê Auto-stop on critical issues\n",
    "    )],\n",
    "    ...\n",
    ")\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "The callback automatically captures metrics and alerts you to problems **as they happen**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93252cec",
   "metadata": {},
   "source": [
    "### Live Warnings Demo (DPO Training)\n",
    "\n",
    "This runs a real DPO training with live PTT warnings. \n",
    "**Note:** This uses a tiny model (tiny-gpt2) for fast demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4d88e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING LIVE WARNINGS + AUTO-DIAGNOSTICS\n",
      "======================================================================\n",
      "\n",
      "This demo shows:\n",
      "  ‚Ä¢ Live warnings printed during training (‚ö†Ô∏è ‚ÑπÔ∏è üö®)\n",
      "  ‚Ä¢ Auto-stop on high-severity issues (if any)\n",
      "  ‚Ä¢ Auto-diagnostics report at end\n",
      "======================================================================\n",
      "\n",
      "[1/4] Loading tiny model: sshleifer/tiny-gpt2\n",
      "   Model parameters: 102,714\n",
      "\n",
      "[2/4] Creating preference dataset...\n",
      "   Train: 48, Eval: 12\n",
      "\n",
      "[3/4] Setting up DPO training with NEW callback options...\n",
      "   Output dir: /home/avasanthc/post-training-toolkit/demo/outputs/test_live_warnings\n",
      "   Options:\n",
      "     ‚Ä¢ enable_live_warnings=True (check every 5 steps)\n",
      "     ‚Ä¢ stop_on_critical=False (for demo)\n",
      "     ‚Ä¢ auto_diagnostics=True (prints summary at end)\n",
      "/home/avasanthc/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "Extracting prompt in train dataset: 100%|‚ñà| 48/48 [00:00<00:00, 3581.24 examples\n",
      "Applying chat template to train dataset: 100%|‚ñà| 48/48 [00:00<00:00, 5340.37 exa\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:00<00:00, 1283.48 examples/s]\n",
      "Extracting prompt in eval dataset: 100%|‚ñà| 12/12 [00:00<00:00, 2382.34 examples/\n",
      "Applying chat template to eval dataset: 100%|‚ñà| 12/12 [00:00<00:00, 3359.03 exam\n",
      "Tokenizing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 1647.57 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\n",
      "[4/4] Training (watch for live warnings!)...\n",
      "======================================================================\n",
      "{'loss': 0.6931, 'grad_norm': 0.01690295897424221, 'learning_rate': 5e-05, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -170.4535369873047, 'logps/rejected': -48.7153205871582, 'logits/chosen': 2.3199372662929818e-05, 'logits/rejected': 3.051331077585928e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6931, 'grad_norm': 0.009334280155599117, 'learning_rate': 4.8611111111111115e-05, 'rewards/chosen': 2.47955313170678e-06, 'rewards/rejected': -4.4822695599577855e-06, 'rewards/accuracies': 1.0, 'rewards/margins': 6.961822691664565e-06, 'logps/chosen': -116.38452911376953, 'logps/rejected': -37.893226623535156, 'logits/chosen': 1.931094448082149e-05, 'logits/rejected': 1.0917431609414052e-05, 'epoch': 0.17}\n",
      "{'loss': 0.6932, 'grad_norm': 0.010692824609577656, 'learning_rate': 4.722222222222222e-05, 'rewards/chosen': -1.4686585018353071e-05, 'rewards/rejected': -1.6212464970521978e-06, 'rewards/accuracies': 0.5, 'rewards/margins': -1.3065338862361386e-05, 'logps/chosen': -132.57470703125, 'logps/rejected': -70.35939025878906, 'logits/chosen': 1.7086289517465048e-05, 'logits/rejected': 2.722765384532977e-05, 'epoch': 0.25}\n",
      "{'loss': 0.6931, 'grad_norm': 0.016822030767798424, 'learning_rate': 4.5833333333333334e-05, 'rewards/chosen': 1.754760705807712e-05, 'rewards/rejected': -7.963180905790068e-06, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5510787963867188e-05, 'logps/chosen': -124.49748229980469, 'logps/rejected': -51.44013214111328, 'logits/chosen': 2.0951942133251578e-05, 'logits/rejected': 1.985729431908112e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6932, 'grad_norm': 0.014575203880667686, 'learning_rate': 4.4444444444444447e-05, 'rewards/chosen': -1.831054760259576e-05, 'rewards/rejected': 7.62939453125e-06, 'rewards/accuracies': 0.5, 'rewards/margins': -2.5939943952835165e-05, 'logps/chosen': -97.42666625976562, 'logps/rejected': -59.510990142822266, 'logits/chosen': 1.569656888023019e-05, 'logits/rejected': 3.183012086083181e-05, 'epoch': 0.42}\n",
      "{'loss': 0.6932, 'grad_norm': 0.01652393490076065, 'learning_rate': 4.305555555555556e-05, 'rewards/chosen': -3.280639793956652e-05, 'rewards/rejected': 9.5367431640625e-07, 'rewards/accuracies': 0.5, 'rewards/margins': -3.376007225597277e-05, 'logps/chosen': -105.506591796875, 'logps/rejected': -51.43022537231445, 'logits/chosen': 2.1596635633613914e-05, 'logits/rejected': 3.330982508487068e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6931, 'grad_norm': 0.004398229066282511, 'learning_rate': 4.166666666666667e-05, 'rewards/chosen': -3.2424925393570447e-06, 'rewards/rejected': -1.1444091796875e-05, 'rewards/accuracies': 0.75, 'rewards/margins': 8.201598575396929e-06, 'logps/chosen': -159.64060974121094, 'logps/rejected': -59.54313278198242, 'logits/chosen': 1.6835270798765123e-05, 'logits/rejected': 1.1902920959983021e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6931, 'grad_norm': 0.006841559428721666, 'learning_rate': 4.027777777777778e-05, 'rewards/chosen': 5.91278057981981e-06, 'rewards/rejected': -7.3432925091765355e-06, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3256073543743696e-05, 'logps/chosen': -110.99603271484375, 'logps/rejected': -40.59445571899414, 'logits/chosen': 1.7473046682425775e-05, 'logits/rejected': 8.145620085997507e-06, 'epoch': 0.67}\n",
      "{'loss': 0.6931, 'grad_norm': 0.0060029649175703526, 'learning_rate': 3.888888888888889e-05, 'rewards/chosen': -3.6239630389900412e-06, 'rewards/rejected': -1.544952465337701e-05, 'rewards/accuracies': 0.75, 'rewards/margins': 1.182556115963962e-05, 'logps/chosen': -137.96054077148438, 'logps/rejected': -54.15085983276367, 'logits/chosen': 1.892258478619624e-05, 'logits/rejected': 1.899258313642349e-05, 'epoch': 0.75}\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 9/36 [00:00<00:02, 10.70it/s]\n",
      "[DiagnosticsCallback] üö® HIGH at step 10: DPO loss stuck at ~0.693 (random chance). Model may not be learning preferences.\n",
      "[DiagnosticsCallback]    Data: mean_recent_loss=0.6931, expected_random=0.6930\n",
      "[DiagnosticsCallback]    Ref: Rafailov et al. (2023) 'DPO', Section 4.2 - Loss at ln(2) indicates no preference signal\n",
      "\n",
      "[DiagnosticsCallback] ‚ö†Ô∏è MEDIUM at step 10: Win rate shows high volatility (std=0.71), indicating inconsistent preference learning.\n",
      "[DiagnosticsCallback]    Data: max_rolling_std=0.7071, window=5\n",
      "{'loss': 0.6932, 'grad_norm': 0.019382743164896965, 'learning_rate': 3.7500000000000003e-05, 'rewards/chosen': -3.395080420887098e-05, 'rewards/rejected': 4.339218321547378e-06, 'rewards/accuracies': 0.5, 'rewards/margins': -3.829002525890246e-05, 'logps/chosen': -124.38067626953125, 'logps/rejected': -48.70159912109375, 'logits/chosen': 2.399492404947523e-05, 'logits/rejected': 3.2921489037107676e-05, 'epoch': 0.83}\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 10/36 [00:00<00:02, 10.70it/s]\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.693136990070343, 'eval_runtime': 0.1546, 'eval_samples_per_second': 77.612, 'eval_steps_per_second': 38.806, 'eval_rewards/chosen': 8.710225301911123e-06, 'eval_rewards/rejected': -1.1634827387752011e-05, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 2.0345052689663135e-05, 'eval_logps/chosen': -113.61100006103516, 'eval_logps/rejected': -35.17285919189453, 'eval_logits/chosen': 2.0225670596119016e-05, 'eval_logits/rejected': 1.3531967852031812e-05, 'epoch': 0.83}\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 10/36 [00:01<00:02, 10.70it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 47.26it/s]\u001b[A\n",
      "{'loss': 0.6931, 'grad_norm': 0.018311141058802605, 'learning_rate': 3.611111111111111e-05, 'rewards/chosen': 2.460479663568549e-05, 'rewards/rejected': -1.8787384760798886e-05, 'rewards/accuracies': 1.0, 'rewards/margins': 4.3392181396484375e-05, 'logps/chosen': -148.8531494140625, 'logps/rejected': -54.12480926513672, 'logits/chosen': 1.5250245269271545e-05, 'logits/rejected': 4.049397375638364e-06, 'epoch': 0.92}\n",
      "{'loss': 0.6932, 'grad_norm': 0.018621817231178284, 'learning_rate': 3.472222222222222e-05, 'rewards/chosen': -2.4604798454674892e-05, 'rewards/rejected': 9.5367431640625e-07, 'rewards/accuracies': 0.5, 'rewards/margins': -2.5558474590070546e-05, 'logps/chosen': -113.599853515625, 'logps/rejected': -48.720794677734375, 'logits/chosen': 1.9771447114180773e-05, 'logits/rejected': 2.5076566089410335e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6932, 'grad_norm': 0.02070571482181549, 'learning_rate': 3.3333333333333335e-05, 'rewards/chosen': -3.528595334501006e-05, 'rewards/rejected': -1.4209747860149946e-05, 'rewards/accuracies': 0.5, 'rewards/margins': -2.107620457536541e-05, 'logps/chosen': -162.216796875, 'logps/rejected': -40.60783386230469, 'logits/chosen': 2.1441621356643736e-05, 'logits/rejected': 2.5927376555046067e-05, 'epoch': 1.08}\n",
      "{'loss': 0.6931, 'grad_norm': 0.00701886834576726, 'learning_rate': 3.194444444444444e-05, 'rewards/chosen': -1.2779237295035273e-05, 'rewards/rejected': -1.2826920283259824e-05, 'rewards/accuracies': 0.75, 'rewards/margins': 4.768344297190197e-08, 'logps/chosen': -156.8349609375, 'logps/rejected': -35.18421936035156, 'logits/chosen': 2.332900112378411e-05, 'logits/rejected': 2.390440749877598e-05, 'epoch': 1.17}\n",
      "{'loss': 0.6932, 'grad_norm': 0.014379634521901608, 'learning_rate': 3.055555555555556e-05, 'rewards/chosen': -1.9645693100756034e-05, 'rewards/rejected': 9.870529538602568e-06, 'rewards/accuracies': 0.5, 'rewards/margins': -2.9516222639358602e-05, 'logps/chosen': -108.22069549560547, 'logps/rejected': -56.8045654296875, 'logits/chosen': 2.111341200361494e-05, 'logits/rejected': 4.012767021777108e-05, 'epoch': 1.25}\n",
      "{'loss': 0.6931, 'grad_norm': 0.004800374619662762, 'learning_rate': 2.916666666666667e-05, 'rewards/chosen': 4.768371127283899e-06, 'rewards/rejected': -7.152557373046875e-06, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1920928955078125e-05, 'logps/chosen': -132.57199096679688, 'logps/rejected': -56.85198211669922, 'logits/chosen': 1.7422757082385942e-05, 'logits/rejected': 1.586340476933401e-05, 'epoch': 1.33}\n",
      "{'loss': 0.6932, 'grad_norm': 0.003001178614795208, 'learning_rate': 2.777777777777778e-05, 'rewards/chosen': -1.2779236385540571e-05, 'rewards/rejected': -7.057190487103071e-06, 'rewards/accuracies': 0.75, 'rewards/margins': -5.7220458984375e-06, 'logps/chosen': -119.0932388305664, 'logps/rejected': -48.701725006103516, 'logits/chosen': 1.8015718524111435e-05, 'logits/rejected': 2.2082387658883817e-05, 'epoch': 1.42}\n",
      "{'loss': 0.6931, 'grad_norm': 0.01142104435712099, 'learning_rate': 2.6388888888888892e-05, 'rewards/chosen': 2.250671423098538e-05, 'rewards/rejected': -5.91278057981981e-06, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8419495720299892e-05, 'logps/chosen': -92.00914001464844, 'logps/rejected': -37.89418029785156, 'logits/chosen': 1.8391509001958184e-05, 'logits/rejected': 7.5613902481563855e-06, 'epoch': 1.5}\n",
      "{'loss': 0.6932, 'grad_norm': 0.005003273021429777, 'learning_rate': 2.5e-05, 'rewards/chosen': -1.1062621524615679e-05, 'rewards/rejected': 3.862380708596902e-06, 'rewards/accuracies': 0.5, 'rewards/margins': -1.4925003597454634e-05, 'logps/chosen': -100.12476348876953, 'logps/rejected': -46.00659942626953, 'logits/chosen': 2.351829971303232e-05, 'logits/rejected': 3.498882142594084e-05, 'epoch': 1.58}\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 19/36 [00:01<00:01, 11.48it/s]\n",
      "[DiagnosticsCallback] ‚ÑπÔ∏è LOW at step 20: DPO loss has plateaued; consider adjusting learning rate or beta.\n",
      "[DiagnosticsCallback]    Data: recent_slope=0.0000\n",
      "{'loss': 0.6931, 'grad_norm': 0.00486638443544507, 'learning_rate': 2.361111111111111e-05, 'rewards/chosen': 8.773804438533261e-06, 'rewards/rejected': -6.67572021484375e-06, 'rewards/accuracies': 0.75, 'rewards/margins': 1.544952465337701e-05, 'logps/chosen': -132.58523559570312, 'logps/rejected': -54.16472625732422, 'logits/chosen': 1.864024125097785e-05, 'logits/rejected': 2.4585362552898005e-05, 'epoch': 1.67}\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 20/36 [00:01<00:01, 11.48it/s]\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6931373476982117, 'eval_runtime': 0.1549, 'eval_samples_per_second': 77.454, 'eval_steps_per_second': 38.727, 'eval_rewards/chosen': 8.328755029651802e-06, 'eval_rewards/rejected': -1.1316936252114829e-05, 'eval_rewards/accuracies': 0.8333333134651184, 'eval_rewards/margins': 1.964569128176663e-05, 'eval_logps/chosen': -113.61100006103516, 'eval_logps/rejected': -35.17285919189453, 'eval_logits/chosen': 2.0208668502164073e-05, 'eval_logits/rejected': 1.3493640835804399e-05, 'epoch': 1.67}\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 20/36 [00:02<00:01, 11.48it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 46.41it/s]\u001b[A\n",
      "{'loss': 0.6931, 'grad_norm': 0.007536470424383879, 'learning_rate': 2.2222222222222223e-05, 'rewards/chosen': 1.5258788153005298e-06, 'rewards/rejected': -3.290176664449973e-06, 'rewards/accuracies': 0.75, 'rewards/margins': 4.816055479750503e-06, 'logps/chosen': -127.17738342285156, 'logps/rejected': -37.8775634765625, 'logits/chosen': 2.19496687350329e-05, 'logits/rejected': 1.3456136002787389e-05, 'epoch': 1.75}\n",
      "{'loss': 0.6931, 'grad_norm': 0.022064682096242905, 'learning_rate': 2.0833333333333336e-05, 'rewards/chosen': 2.6130677724722773e-05, 'rewards/rejected': -4.673003786592744e-06, 'rewards/accuracies': 1.0, 'rewards/margins': 3.080368333030492e-05, 'logps/chosen': -135.38436889648438, 'logps/rejected': -73.0860824584961, 'logits/chosen': 1.2990865798201412e-05, 'logits/rejected': 1.307758066104725e-05, 'epoch': 1.83}\n",
      "{'loss': 0.6932, 'grad_norm': 0.010627309791743755, 'learning_rate': 1.9444444444444445e-05, 'rewards/chosen': -5.7220458984375e-06, 'rewards/rejected': 2.7656558359012706e-06, 'rewards/accuracies': 0.5, 'rewards/margins': -8.487701961712446e-06, 'logps/chosen': -132.57460021972656, 'logps/rejected': -70.35934448242188, 'logits/chosen': 1.6445652363472618e-05, 'logits/rejected': 2.842916364897974e-05, 'epoch': 1.92}\n",
      "{'loss': 0.6931, 'grad_norm': 0.0064228191040456295, 'learning_rate': 1.8055555555555555e-05, 'rewards/chosen': 1.0490418389963452e-05, 'rewards/rejected': -1.2397767932270654e-06, 'rewards/accuracies': 0.75, 'rewards/margins': 1.173019700217992e-05, 'logps/chosen': -143.48062133789062, 'logps/rejected': -67.64604949951172, 'logits/chosen': 1.4755657502973918e-05, 'logits/rejected': 2.0862677047261968e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6931, 'grad_norm': 0.01795986481010914, 'learning_rate': 1.6666666666666667e-05, 'rewards/chosen': -7.057191396597773e-06, 'rewards/rejected': -7.05719003235572e-06, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0, 'logps/chosen': -118.97486114501953, 'logps/rejected': -48.70691680908203, 'logits/chosen': 1.98889356397558e-05, 'logits/rejected': 2.426734135951847e-05, 'epoch': 2.08}\n",
      "{'loss': 0.6931, 'grad_norm': 0.021856948733329773, 'learning_rate': 1.527777777777778e-05, 'rewards/chosen': -5.7220458984375e-06, 'rewards/rejected': -4.720688139059348e-06, 'rewards/accuracies': 0.5, 'rewards/margins': -1.001357759378152e-06, 'logps/chosen': -145.95408630371094, 'logps/rejected': -35.21372985839844, 'logits/chosen': 2.6001194783020765e-05, 'logits/rejected': 4.1752275137696415e-05, 'epoch': 2.17}\n",
      "{'loss': 0.6932, 'grad_norm': 0.009000387042760849, 'learning_rate': 1.388888888888889e-05, 'rewards/chosen': -1.640319896978326e-05, 'rewards/rejected': 1.068115216185106e-05, 'rewards/accuracies': 0.5, 'rewards/margins': -2.7084352041129023e-05, 'logps/chosen': -111.0143051147461, 'logps/rejected': -81.15997314453125, 'logits/chosen': 1.3801832210447174e-05, 'logits/rejected': 3.091298640356399e-05, 'epoch': 2.25}\n",
      "{'loss': 0.6931, 'grad_norm': 0.014688410796225071, 'learning_rate': 1.25e-05, 'rewards/chosen': 2.765655517578125e-05, 'rewards/rejected': -4.625320798368193e-06, 'rewards/accuracies': 1.0, 'rewards/margins': 3.228187415516004e-05, 'logps/chosen': -132.5774383544922, 'logps/rejected': -51.41790771484375, 'logits/chosen': 1.8766033463180065e-05, 'logits/rejected': 1.2568185411510058e-05, 'epoch': 2.33}\n",
      "{'loss': 0.6931, 'grad_norm': 0.02614864520728588, 'learning_rate': 1.1111111111111112e-05, 'rewards/chosen': 3.738403393072076e-05, 'rewards/rejected': -6.675720669591101e-06, 'rewards/accuracies': 1.0, 'rewards/margins': 4.405975414556451e-05, 'logps/chosen': -167.85336303710938, 'logps/rejected': -73.07952117919922, 'logits/chosen': 1.3610362657345831e-05, 'logits/rejected': 4.380641257739626e-06, 'epoch': 2.42}\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 30/36 [00:03<00:00,  9.01it/s]\n",
      "[DiagnosticsCallback] üö® HIGH at step 30: Reward margin collapsed to 0.000\n",
      "[DiagnosticsCallback]    Data: metric=reward_margin, condition=< 0.1, value=0.0000\n",
      "[DiagnosticsCallback]    Ref: https://arxiv.org/abs/2305.18290\n",
      "\n",
      "[DiagnosticsCallback] üö® HIGH at step 30: DPO loss stuck at 0.6931 (random chance ~0.693). Model may not be learning preferences.\n",
      "[DiagnosticsCallback]    Data: metric=dpo_loss, condition=range(0.68, 0.71), value=0.6931\n",
      "[DiagnosticsCallback]    Ref: Rafailov et al. (2023) 'DPO', Section 4.2 - Loss at ln(2) indicates no preference signal\n",
      "{'loss': 0.6931, 'grad_norm': 0.005786114372313023, 'learning_rate': 9.722222222222223e-06, 'rewards/chosen': 1.0299681889591739e-05, 'rewards/rejected': -1.1444091796875e-05, 'rewards/accuracies': 0.75, 'rewards/margins': 2.174377368646674e-05, 'logps/chosen': -137.94712829589844, 'logps/rejected': -56.83806610107422, 'logits/chosen': 1.760238956194371e-05, 'logits/rejected': 2.192667670897208e-05, 'epoch': 2.5}\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 30/36 [00:03<00:00,  9.01it/s]\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6931362152099609, 'eval_runtime': 0.1442, 'eval_samples_per_second': 83.228, 'eval_steps_per_second': 41.614, 'eval_rewards/chosen': 9.600321391189937e-06, 'eval_rewards/rejected': -1.2270610568521079e-05, 'eval_rewards/accuracies': 0.8333333134651184, 'eval_rewards/margins': 2.1870931959711015e-05, 'eval_logps/chosen': -113.61100006103516, 'eval_logps/rejected': -35.17286682128906, 'eval_logits/chosen': 2.020644569711294e-05, 'eval_logits/rejected': 1.3484794180840254e-05, 'epoch': 2.5}\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 30/36 [00:03<00:00,  9.01it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 56.79it/s]\u001b[A\n",
      "{'loss': 0.6932, 'grad_norm': 0.005638075992465019, 'learning_rate': 8.333333333333334e-06, 'rewards/chosen': -3.2424923119833693e-06, 'rewards/rejected': 8.296967280330136e-06, 'rewards/accuracies': 0.5, 'rewards/margins': -1.1539457773324102e-05, 'logps/chosen': -116.41474151611328, 'logps/rejected': -78.46231079101562, 'logits/chosen': 1.4327739336295053e-05, 'logits/rejected': 2.8440314054023474e-05, 'epoch': 2.58}\n",
      "{'loss': 0.6931, 'grad_norm': 0.009938392788171768, 'learning_rate': 6.944444444444445e-06, 'rewards/chosen': 1.9073486328125e-06, 'rewards/rejected': -8.96453821042087e-06, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0871887752728071e-05, 'logps/chosen': -108.20979309082031, 'logps/rejected': -35.223876953125, 'logits/chosen': 2.154031244572252e-05, 'logits/rejected': 2.453624620102346e-05, 'epoch': 2.67}\n",
      "{'loss': 0.6931, 'grad_norm': 0.010521713644266129, 'learning_rate': 5.555555555555556e-06, 'rewards/chosen': -3.8146970382513246e-07, 'rewards/rejected': -3.43322744811303e-06, 'rewards/accuracies': 0.5, 'rewards/margins': 3.051758312722086e-06, 'logps/chosen': -108.30332946777344, 'logps/rejected': -40.60615921020508, 'logits/chosen': 1.9549097487470135e-05, 'logits/rejected': 1.3773777027381584e-05, 'epoch': 2.75}\n",
      "{'loss': 0.6932, 'grad_norm': 0.0054926336742937565, 'learning_rate': 4.166666666666667e-06, 'rewards/chosen': -1.220703143189894e-05, 'rewards/rejected': -3.814700448856456e-07, 'rewards/accuracies': 0.75, 'rewards/margins': -1.182556115963962e-05, 'logps/chosen': -113.70477294921875, 'logps/rejected': -51.40285873413086, 'logits/chosen': 1.65273631864693e-05, 'logits/rejected': 1.8951473975903355e-05, 'epoch': 2.83}\n",
      "{'loss': 0.6931, 'grad_norm': 0.00947917252779007, 'learning_rate': 2.777777777777778e-06, 'rewards/chosen': 0.0, 'rewards/rejected': -1.5115738278836943e-05, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5115738278836943e-05, 'logps/chosen': -162.34043884277344, 'logps/rejected': -35.168601989746094, 'logits/chosen': 2.0540435798466206e-05, 'logits/rejected': 9.156830856227316e-06, 'epoch': 2.92}\n",
      "{'loss': 0.6931, 'grad_norm': 0.0036932267248630524, 'learning_rate': 1.388888888888889e-06, 'rewards/chosen': -3.814697265625e-06, 'rewards/rejected': 1.0013579867518274e-06, 'rewards/accuracies': 0.25, 'rewards/margins': -4.816055479750503e-06, 'logps/chosen': -118.97904968261719, 'logps/rejected': -37.90489959716797, 'logits/chosen': 2.6793750294018537e-05, 'logits/rejected': 3.2982909033307806e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 3.9395, 'train_samples_per_second': 36.552, 'train_steps_per_second': 9.138, 'train_loss': 0.6931462006436454, 'epoch': 3.0}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:03<00:00,  8.28it/s]\n",
      "======================================================================\n",
      "POST-TRAINING TOOLKIT - DIAGNOSTICS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Trainer: DPO | Steps: 36\n",
      "Artifacts: /home/avasanthc/post-training-toolkit/demo/outputs/test_live_warnings\n",
      "\n",
      "--- Performance Profile ---\n",
      "  Total time: 3.2s\n",
      "  Mean step time: 90ms\n",
      "  ‚ö†Ô∏è Slowdown: 29% slower than start\n",
      "  Throughput: 3,005 tokens/sec\n",
      "\n",
      "--- Training Metrics ---\n",
      "  DPO Loss: 0.6931 (started: 0.6931)\n",
      "  Win Rate: nan% (mean: 68.1%)\n",
      "  Reward Margin: nan\n",
      "\n",
      "--- Diagnostics ---\n",
      "\n",
      "  üö® HIGH SEVERITY (3):\n",
      "     ‚Ä¢ DPO loss stuck at ~0.693 (random chance). Model may not be learning preferences.\n",
      "       Ref: Rafailov et al. (2023) 'DPO', Section 4.2 - Loss at ln(2) indicates no preference signal\n",
      "     ‚Ä¢ Reward margin collapsed to 0.000\n",
      "       Ref: https://arxiv.org/abs/2305.18290\n",
      "     ‚Ä¢ DPO loss stuck at 0.6931 (random chance ~0.693). Model may not be learning preferences.\n",
      "       Ref: Rafailov et al. (2023) 'DPO', Section 4.2 - Loss at ln(2) indicates no preference signal\n",
      "\n",
      "  ‚ö†Ô∏è  MEDIUM (1):\n",
      "     ‚Ä¢ Win rate shows high volatility (std=0.71), indicating inconsistent preference learning.\n",
      "\n",
      "  ‚ÑπÔ∏è  LOW (1):\n",
      "     ‚Ä¢ DPO loss has plateaued; consider adjusting learning rate or beta.\n",
      "\n",
      "--- Recommendations ---\n",
      "  ‚Üí DPO loss at random chance: increase learning rate 2-5x, check data quality, or reduce beta\n",
      "\n",
      "======================================================================\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:03<00:00,  9.15it/s]\n",
      "\n",
      "‚úÖ Detected trainer type: DPO\n",
      "   Metrics saved to: /home/avasanthc/post-training-toolkit/demo/outputs/test_live_warnings/metrics.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Run DPO training with live PTT warnings\n",
    "# Watch the output for live alerts like: [DiagnosticsCallback] ‚ö†Ô∏è MEDIUM at step 20: ...\n",
    "\n",
    "!cd ../.. && python demo/scripts/test_live_warnings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c998cf",
   "metadata": {},
   "source": [
    "### Auto-Stop Demo (Critical Issue Detection)\n",
    "\n",
    "This demo shows PTT's **auto-stop** feature. We intentionally inject a bug:\n",
    "- **Bug**: Learning rate is 100x too high (5e-3 instead of 5e-5)\n",
    "- **Effect**: DPO loss gets stuck at 0.693 (random chance = not learning)\n",
    "- **PTT Response**: Detects critical issue and **auto-stops training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57c77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCENARIO 2: REAL DPO TRAINING WITH PTT AUTO-STOP\n",
      "======================================================================\n",
      "\n",
      "Setup:\n",
      "  ‚Ä¢ Model: gpt2 (124M)\n",
      "  ‚Ä¢ Trainer: DPO\n",
      "  ‚Ä¢ Dataset: Anthropic/hh-rlhf (25 examples)\n",
      "  ‚Ä¢ Bug: Very high LR (5e-3) ‚Üí DPO loss explodes (>2.0)\n",
      "  ‚Ä¢ PTT: stop_on_critical=True (AUTO-STOP ENABLED)\n",
      "  ‚Ä¢ Expected: Auto-stop when loss exceeds 2.0\n",
      "\n",
      "Loading model and tokenizer...\n",
      "Loading dataset...\n",
      "\n",
      "Configuring DPO training...\n",
      "\n",
      "Creating DPO trainer with PTT callback...\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING - PTT WILL AUTO-STOP ON CRITICAL ISSUES\n",
      "======================================================================\n",
      "\n",
      "üëÄ PTT will check metrics every step (logging_steps=1)\n",
      "üö® Expected: Critical alert + AUTO-STOP around step 11 when loss > 2.0\n",
      "\n",
      "[DiagnosticsCallback] Detected trainer: DPO\n",
      "[DiagnosticsCallback] Run directory: real_scenario2_output/diagnostics\n",
      "[DiagnosticsCallback] Auto-diff enabled\n",
      "[DiagnosticsCallback] Postmortem recording enabled\n",
      "[DiagnosticsCallback] Safe stopping enabled (will stop on NaN/Inf)\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]/home/avasanthc/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "  5%|‚ñà‚ñà                                          | 1/21 [00:06<02:08,  6.42s/it][DiagnosticsCallback] Step 1 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 0.6931, 'grad_norm': 50.42881774902344, 'learning_rate': 0.005, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -67.25789642333984, 'logps/rejected': -123.48382568359375, 'logits/chosen': -123.42433166503906, 'logits/rejected': -101.34712219238281, 'epoch': 0.16}\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                       | 2/21 [00:10<01:32,  4.88s/it][DiagnosticsCallback] Step 2 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 10.5007, 'grad_norm': 74.80536651611328, 'learning_rate': 0.0047619047619047615, 'rewards/chosen': -14.982131958007812, 'rewards/rejected': -4.502029895782471, 'rewards/accuracies': 0.0, 'rewards/margins': -10.480100631713867, 'logps/chosen': -257.57275390625, 'logps/rejected': -100.8558349609375, 'logits/chosen': -37.48906707763672, 'logits/rejected': -39.5098762512207, 'epoch': 0.32}\n",
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                     | 3/21 [00:15<01:32,  5.14s/it][DiagnosticsCallback] Step 3 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 10.6833, 'grad_norm': 33.68658447265625, 'learning_rate': 0.004523809523809524, 'rewards/chosen': -20.351587295532227, 'rewards/rejected': -17.967546463012695, 'rewards/accuracies': 0.75, 'rewards/margins': -2.3840372562408447, 'logps/chosen': -309.4060974121094, 'logps/rejected': -290.0813903808594, 'logits/chosen': -23.03982925415039, 'logits/rejected': -23.053068161010742, 'epoch': 0.48}\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 4/21 [00:22<01:37,  5.72s/it][DiagnosticsCallback] Step 4 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 10.2689, 'grad_norm': 111.44619750976562, 'learning_rate': 0.004285714285714286, 'rewards/chosen': -32.09098434448242, 'rewards/rejected': -60.3562126159668, 'rewards/accuracies': 0.75, 'rewards/margins': 28.265228271484375, 'logps/chosen': -405.0280456542969, 'logps/rejected': -744.7835693359375, 'logits/chosen': 42.2282600402832, 'logits/rejected': 41.139549255371094, 'epoch': 0.64}\n",
      " 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 5/21 [00:30<01:43,  6.46s/it][DiagnosticsCallback] Step 5 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 0.6847, 'grad_norm': 6.215994358062744, 'learning_rate': 0.004047619047619048, 'rewards/chosen': -9.06175422668457, 'rewards/rejected': -36.422061920166016, 'rewards/accuracies': 0.75, 'rewards/margins': 27.360309600830078, 'logps/chosen': -186.8067626953125, 'logps/rejected': -653.6217041015625, 'logits/chosen': -11.507501602172852, 'logits/rejected': -9.589194297790527, 'epoch': 0.8}\n",
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 6/21 [00:33<01:21,  5.46s/it][DiagnosticsCallback] Step 6 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 0.4277, 'grad_norm': 12.783669471740723, 'learning_rate': 0.0038095238095238095, 'rewards/chosen': -11.397846221923828, 'rewards/rejected': -15.484994888305664, 'rewards/accuracies': 0.75, 'rewards/margins': 4.087148189544678, 'logps/chosen': -183.92820739746094, 'logps/rejected': -238.27142333984375, 'logits/chosen': -37.11311340332031, 'logits/rejected': -44.24473190307617, 'epoch': 0.96}\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 7/21 [00:40<01:22,  5.88s/it][DiagnosticsCallback] Step 7 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0035714285714285718, 'rewards/chosen': -8.531089782714844, 'rewards/rejected': -75.07591247558594, 'rewards/accuracies': 1.0, 'rewards/margins': 66.5448226928711, 'logps/chosen': -132.71926879882812, 'logps/rejected': -1056.453369140625, 'logits/chosen': -25.78933334350586, 'logits/rejected': -31.896726608276367, 'epoch': 1.0}\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 8/21 [00:58<02:09,  9.94s/it][DiagnosticsCallback] Step 8 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 0.0, 'grad_norm': 1.7730999957166205e-07, 'learning_rate': 0.003333333333333333, 'rewards/chosen': -9.962736129760742, 'rewards/rejected': -46.19003677368164, 'rewards/accuracies': 1.0, 'rewards/margins': 36.22730255126953, 'logps/chosen': -176.13723754882812, 'logps/rejected': -598.4964599609375, 'logits/chosen': -27.142250061035156, 'logits/rejected': -34.71397399902344, 'epoch': 1.16}\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 9/21 [01:38<03:51, 19.32s/it][DiagnosticsCallback] Step 9 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 0.0, 'grad_norm': 9.444831448490731e-06, 'learning_rate': 0.0030952380952380953, 'rewards/chosen': -9.702997207641602, 'rewards/rejected': -60.74217987060547, 'rewards/accuracies': 1.0, 'rewards/margins': 51.0391845703125, 'logps/chosen': -138.53826904296875, 'logps/rejected': -859.5715942382812, 'logits/chosen': -25.11241912841797, 'logits/rejected': -27.82785415649414, 'epoch': 1.32}\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 10/21 [01:45<02:50, 15.50s/it]\n",
      "[DiagnosticsCallback] ‚ö†Ô∏è MEDIUM at step 10: Win rate shows high volatility (std=0.43), indicating inconsistent preference learning.\n",
      "[DiagnosticsCallback]    Data: max_rolling_std=0.4330, window=5\n",
      "[DiagnosticsCallback] Step 10 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 12.3794, 'grad_norm': 32.95277404785156, 'learning_rate': 0.002857142857142857, 'rewards/chosen': -44.39528274536133, 'rewards/rejected': -50.08479309082031, 'rewards/accuracies': 0.75, 'rewards/margins': 5.689513206481934, 'logps/chosen': -565.8951416015625, 'logps/rejected': -704.7091064453125, 'logits/chosen': -30.74698257446289, 'logits/rejected': -29.660673141479492, 'epoch': 1.48}\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 11/21 [01:49<01:58, 11.87s/it][DiagnosticsCallback] Step 11 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 1.6606, 'grad_norm': 6.954235553741455, 'learning_rate': 0.0026190476190476194, 'rewards/chosen': -8.047966003417969, 'rewards/rejected': -12.451395034790039, 'rewards/accuracies': 0.5, 'rewards/margins': 4.40342903137207, 'logps/chosen': -144.45098876953125, 'logps/rejected': -200.1855926513672, 'logits/chosen': -17.25009536743164, 'logits/rejected': -17.44466781616211, 'epoch': 1.64}\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 12/21 [01:52<01:23,  9.30s/it]\n",
      "[DiagnosticsCallback] üö® HIGH at step 12: DPO loss exploded! Normal range is 0.5-1.0. High LR is destabilizing training.\n",
      "[DiagnosticsCallback]    Data: metric=dpo_loss, condition=> 2.0, value=3.2177\n",
      "\n",
      "[DiagnosticsCallback] üõë STOPPING TRAINING due to high-severity issue\n",
      "[DiagnosticsCallback]    Reason: DPO loss exploded! Normal range is 0.5-1.0. High LR is destabilizing training.\n",
      "[DiagnosticsCallback]    Last good step: 12\n",
      "[DiagnosticsCallback] Step 12 (dpo): ['reward_mean', 'reward_std', 'dpo_loss', 'win_rate', 'reward_margin', 'logps_chosen', 'logps_rejected', 'logprobs', 'rewards_chosen', 'rewards_rejected']\n",
      "{'loss': 3.2177, 'grad_norm': 7.3522725105285645, 'learning_rate': 0.0023809523809523807, 'rewards/chosen': -6.521515846252441, 'rewards/rejected': -4.030437469482422, 'rewards/accuracies': 0.25, 'rewards/margins': -2.4910788536071777, 'logps/chosen': -149.03768920898438, 'logps/rejected': -77.93800354003906, 'logits/chosen': -18.218765258789062, 'logits/rejected': -18.158615112304688, 'epoch': 1.8}\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 12/21 [01:52<01:23,  9.30s/it]\n",
      "[DiagnosticsCallback] üõë STOPPING TRAINING due to high-severity issue\n",
      "[DiagnosticsCallback]    Reason: DPO loss exploded! Normal range is 0.5-1.0. High LR is destabilizing training.\n",
      "[DiagnosticsCallback]    Last good step: 12\n",
      "[DiagnosticsCallback] Step 12 (dpo): ['dpo_loss']\n",
      "{'train_runtime': 112.9383, 'train_samples_per_second': 0.664, 'train_steps_per_second': 0.186, 'train_loss': 4.209673856799956, 'epoch': 1.8}\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 12/21 [01:52<01:23,  9.30s/it][DiagnosticsCallback] Run ended due to critical failure: inline_dpo_loss_gt: DPO loss exploded! Normal range is 0.5-1.0. High LR is destabilizing training.\n",
      "[DiagnosticsCallback] Recommend resuming from step 12\n",
      "[DiagnosticsCallback] Training complete (dpo). Artifacts in real_scenario2_output/diagnostics\n",
      "\n",
      "======================================================================\n",
      "POST-TRAINING TOOLKIT - DIAGNOSTICS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Trainer: DPO | Steps: 12\n",
      "Artifacts: real_scenario2_output/diagnostics\n",
      "\n",
      "--- Performance Profile ---\n",
      "  Total time: 110.5s\n",
      "  Mean step time: 9211ms\n",
      "  ‚úì No significant slowdown detected\n",
      "  Throughput: 45 tokens/sec\n",
      "\n",
      "--- Training Metrics ---\n",
      "  DPO Loss: 4.2097 (started: 0.6931)\n",
      "  Win Rate: nan% (mean: 62.5%)\n",
      "  Reward Margin: nan\n",
      "\n",
      "--- Diagnostics ---\n",
      "\n",
      "  ‚ö†Ô∏è  MEDIUM (1):\n",
      "     ‚Ä¢ Win rate shows high volatility (std=0.43), indicating inconsistent preference learning.\n",
      "\n",
      "--- Recommendations ---\n",
      "  ‚Üí Training looks healthy. Review metrics history for fine-tuning.\n",
      "\n",
      "======================================================================\n",
      "\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 12/21 [01:52<01:24,  9.41s/it]\n",
      "\n",
      "‚úÖ Training completed (if you see this, bug didn't trigger)\n",
      "\n",
      "======================================================================\n",
      "TRAINING ENDED\n",
      "======================================================================\n",
      "\n",
      "üõë PTT AUTO-STOPPED TRAINING\n",
      "   Reason: inline_dpo_loss_gt: DPO loss exploded! Normal range is 0.5-1.0. High LR is destabilizing training.\n",
      "   Step when stopped: 12\n",
      "   Last good step: 12\n",
      "   Compute saved: ~6 steps\n",
      "\n",
      "üí∞ In a real GPU run at $2.50/hr with 8 GPUs:\n",
      "   This would have saved: $3+\n",
      "\n",
      "üìä Metrics at Stop:\n",
      "   Loss: 4.2097 (should be < 0.5, stuck at ~0.693 = random)\n",
      "   Margin: nan\n",
      "   Win Rate: nan%\n",
      "\n",
      "üìÅ Artifacts saved to: real_scenario2_output/diagnostics\n",
      "   ‚Ä¢ Metrics log: real_scenario2_output/diagnostics/metrics.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run DPO training with intentional bug - PTT will auto-stop\n",
    "# Expected: Critical alert when loss stuck at 0.693, then auto-stop\n",
    "\n",
    "!cd ../.. && python demo/real_scenario2_dpo_autostop.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371a872",
   "metadata": {},
   "source": [
    "### View Generated Diagnostics Report\n",
    "\n",
    "After training, PTT generates a markdown report with:\n",
    "- Training status (Stable / Partially unstable / Unstable)\n",
    "- Key insights ranked by severity\n",
    "- Recommended actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1ec29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Showing report: ../outputs/reports/diagnostics_log_report.md\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## RLHF Run Diagnostic Report\n",
       "\n",
       "Generated: 2026-01-22T06:14:16.026655Z\n",
       "\n",
       "**Trainer:** DPO | **Status:** Crashed (exception)\n",
       "\n",
       "### Run Summary\n",
       "- Steps: 24\n",
       "\n",
       "- Final DPO Loss: 0.6931\n",
       "- Mean Win Rate: 65.6%\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "### Key Insights\n",
       "\n",
       "\n",
       "1. [HIGH] DPO loss stuck at ~0.693 (random chance). Model may not be learning preferences.\n",
       "   *Ref: Rafailov et al. (2023) 'DPO', Section 4.2 - Loss at ln(2) indicates no preference signal*\n",
       "\n",
       "2. [MEDIUM] Win rate shows high volatility (std=0.53), indicating inconsistent preference learning.\n",
       "\n",
       "3. [LOW] DPO loss has plateaued; consider adjusting learning rate or beta.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "### Postmortem\n",
       "**Exit Reason:** exception\n",
       "- Last Step: 15\n",
       "- Timestamp: 2025-12-17T19:26:04.880887+00:00\n",
       "\n",
       "\n",
       "### Recommended Actions\n",
       "\n",
       "\n",
       "- DPO loss at random chance: increase learning rate 2-5x, check data quality, or reduce beta.\n",
       "\n",
       "- DPO loss plateaued: try learning rate warmup/decay or adjust beta parameter.\n",
       "\n",
       "- Win rate unstable: increase batch size for more stable gradient estimates.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the most recent diagnostics report\n",
    "report_paths = [\n",
    "    Path(\"../outputs/test_live_warnings/diagnostics_report.md\"),\n",
    "    Path(\"../outputs/reports\").glob(\"*_report.md\"),\n",
    "]\n",
    "\n",
    "report_found = False\n",
    "for p in report_paths:\n",
    "    if isinstance(p, Path) and p.exists():\n",
    "        print(f\"üìÑ Showing report: {p}\")\n",
    "        display(Markdown(p.read_text()))\n",
    "        report_found = True\n",
    "        break\n",
    "    elif hasattr(p, '__iter__'):\n",
    "        for f in sorted(p, key=lambda x: x.stat().st_mtime, reverse=True):\n",
    "            print(f\"üìÑ Showing report: {f}\")\n",
    "            display(Markdown(f.read_text()))\n",
    "            report_found = True\n",
    "            break\n",
    "    if report_found:\n",
    "        break\n",
    "\n",
    "if not report_found:\n",
    "    print(\"No diagnostics report found. Run the training cells above first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53243e",
   "metadata": {},
   "source": [
    "### Callback Configuration Options\n",
    "\n",
    "```python\n",
    "DiagnosticsCallback(\n",
    "    # Where to save diagnostics\n",
    "    run_dir=\"./my_run\",\n",
    "    \n",
    "    # Live warnings during training\n",
    "    enable_live_warnings=True,     # Print alerts as they happen\n",
    "    live_warning_interval=10,      # Check every N steps\n",
    "    \n",
    "    # Auto-stop on critical issues\n",
    "    stop_on_critical=True,         # Stop training on HIGH severity\n",
    "    \n",
    "    # End-of-run diagnostics\n",
    "    auto_diagnostics=True,         # Print summary when training ends\n",
    "    \n",
    "    # Debug output\n",
    "    verbose=True,                  # Extra logging\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5536d35",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Contributing\n",
    "\n",
    "### Adding a heuristic = Writing a YAML file\n",
    "\n",
    "Here's the heuristic that caught the IS ratio bug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a builtin heuristic - this is all it takes!\n",
    "from pathlib import Path\n",
    "\n",
    "yaml_path = Path(\"../../post_training_toolkit/heuristics/builtin/dpo/loss_random.yaml\")\n",
    "print(\"üìÑ Builtin heuristic example:\")\n",
    "print(\"-\" * 50)\n",
    "print(yaml_path.read_text())\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n‚ú® That's it! 12 lines of YAML = a training diagnostic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddce96",
   "metadata": {},
   "source": [
    "### The Condition DSL\n",
    "\n",
    "| Syntax | Meaning | Example Use Case |\n",
    "|--------|---------|------------------|\n",
    "| `< 0.5` | Below threshold | Reward margin too low |\n",
    "| `> 2.0` | Above threshold | KL divergence explosion |\n",
    "| `range(0.68, 0.71)` | Stuck in range | DPO loss at random chance |\n",
    "| `drop(50%)` | Dropped 50% from baseline | Entropy collapse |\n",
    "| `spike(3x)` | 3x above rolling average | Loss oscillation |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Live Demo: Create & Use a Custom Heuristic\n",
    "\n",
    "Let's create our own heuristic and watch PTT use it in real training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a custom heuristic YAML file\n",
    "from pathlib import Path\n",
    "\n",
    "custom_dir = Path(\"../custom_heuristics/dpo\")\n",
    "custom_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Note: Use PTT's standard metric names (e.g., \"reward_margin\")\n",
    "yaml_content = \"\"\"# Custom heuristic: Detect when reward margin is too low\n",
    "name: custom_margin_too_low\n",
    "description: Detect when model can't distinguish chosen vs rejected\n",
    "trainers: [dpo]\n",
    "metric: reward_margin\n",
    "condition: \"< 0.02\"\n",
    "window: 5\n",
    "severity: high\n",
    "message: \"Reward margin too low ({value:.4f}). Model can't distinguish chosen vs rejected!\"\n",
    "min_steps: 5\n",
    "enabled: true\n",
    "\"\"\"\n",
    "\n",
    "yaml_path = custom_dir / \"margin_too_low.yaml\"  # Give it a descriptive name\n",
    "yaml_path.write_text(yaml_content)\n",
    "\n",
    "print(\"‚úÖ Created custom heuristic!\")\n",
    "print(f\"üìÑ File: {yaml_path}\")\n",
    "print()\n",
    "print(\"Contents:\")\n",
    "print(\"-\" * 50)\n",
    "print(yaml_content)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304cd3f2",
   "metadata": {},
   "source": [
    "**Step 2: Run training and watch PTT auto-stop!**\n",
    "\n",
    "We'll run DPO training with a tiny model + `stop_on_critical=True`. \n",
    "\n",
    "When our custom heuristic fires (HIGH severity), PTT will:\n",
    "1. Print the alert: `üö® HIGH: Reward margin too low`\n",
    "2. **Automatically stop training** to save compute!\n",
    "\n",
    "Watch for the auto-stop around step 20!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f81fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Run training with our custom heuristic + auto-stop\n",
    "# Watch for: üö® HIGH alert ‚Üí then training stops automatically!\n",
    "\n",
    "!cd ../.. && python demo/scripts/custom_heuristic_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f9516",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "**The missing layer for continuous autonomous RL.**  \n",
    "Post Training Toolkit turns continuous training from a fragile, manual process into a system that can be monitored and controlled reliably.\n",
    "\n",
    "**Built for agents.**  \n",
    "PTT exposes training signals in a structured, machine-readable form that agents can reason over directly. Instead of parsing logs, agents can inspect diagnostics, detect degeneracy early, and propose fixes while training is still running.\n",
    "\n",
    "This is the foundation for self-correcting, continuously trained agent systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
